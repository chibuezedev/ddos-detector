{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chibuezedev/ddos-detector/blob/main/ddos_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask[dataframe]\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Y99SPzv5Ro",
        "outputId": "03fd2b50-c83b-4dee-f243-8be1e14a2e77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.5.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (17.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.21.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->dask[dataframe]) (2024.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n",
            "Downloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NetworkDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "pMzCPkSLy0GM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(CNN1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "        # correct input size for the first fully connected layer\n",
        "        self.flatten_size = 64 * ((input_size // 4))\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VDMNSiGPy5aN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data_df):\n",
        "    essential_features = [\n",
        "        'Packets', 'Bytes', 'Tx Packets', 'Tx Bytes',\n",
        "        'Rx Packets', 'Rx Bytes', 'tcp.srcport', 'tcp.dstport',\n",
        "        'ip.proto', 'frame.len'\n",
        "    ]\n",
        "\n",
        "    features = data_df[essential_features].values\n",
        "    labels = pd.Categorical(data_df['Label']).codes\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    return features_scaled, labels, scaler"
      ],
      "metadata": {
        "id": "KNbvvCmiy9ks"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_random_forest(X_train, X_test, y_train, y_test):\n",
        "    print(\"\\nTraining Random Forest...\")\n",
        "\n",
        "    # Hyperparameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    rf_random = RandomizedSearchCV(rf, param_grid, n_iter=10, cv=3, random_state=42)\n",
        "    rf_random.fit(X_train, y_train)\n",
        "\n",
        "    best_rf = rf_random.best_estimator_\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "\n",
        "    print(\"\\nRandom Forest Results:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': [f'Feature_{i}' for i in range(X_train.shape[1])],\n",
        "        'importance': best_rf.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    return {\n",
        "        'model': best_rf,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'feature_importance': feature_importance,\n",
        "        'predictions': y_pred,\n",
        "        'best_params': rf_random.best_params_\n",
        "    }"
      ],
      "metadata": {
        "id": "mC6towirzDRd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lightgbm(X_train, X_test, y_train, y_test):\n",
        "    print(\"\\nTraining LightGBM...\")\n",
        "\n",
        "    # dataset for LightGBM\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "    # Parameters\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(y_train)),\n",
        "        'metric': 'multi_logloss',\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5\n",
        "    }\n",
        "\n",
        "    # Train model\n",
        "    model = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    print(\"\\nLightGBM Results:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'predictions': y_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "EJfAUwKizKUe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cnn(X_train, X_test, y_train, y_test, num_classes):\n",
        "    print(\"\\nTraining CNN...\")\n",
        "\n",
        "    train_dataset = NetworkDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    model = CNN1D(input_size, num_classes)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    num_epochs = 10\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "        outputs = model(X_test_tensor)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        y_pred = predicted.cpu().numpy()\n",
        "\n",
        "    print(\"\\nCNN Results:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'predictions': y_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "YRb6tWFRzQ0e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(results):\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    accuracies = {model: results[model]['accuracy'] for model in results}\n",
        "    for model, acc in accuracies.items():\n",
        "        print(f\"{model} Accuracy: {acc:.4f}\")\n",
        "\n",
        "    best_model = max(accuracies.items(), key=lambda x: x[1])[0]\n",
        "    print(f\"\\nBest performing model: {best_model} with accuracy: {accuracies[best_model]:.4f}\")\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "sVTX-unBzZVd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_best_model(results, best_model_name):\n",
        "    output_dir = './best_model'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model = results[best_model_name]['model']\n",
        "\n",
        "    if best_model_name == 'Random Forest':\n",
        "        joblib.dump(model, f'{output_dir}/random_forest.joblib')\n",
        "    elif best_model_name == 'LightGBM':\n",
        "        model.save_model(f'{output_dir}/lightgbm_model.txt')\n",
        "    elif best_model_name == 'CNN':\n",
        "        torch.save(model.state_dict(), f'{output_dir}/cnn_model.pt')\n",
        "\n",
        "    # Save model info\n",
        "    model_info = {\n",
        "        'best_model': best_model_name,\n",
        "        'accuracy': results[best_model_name]['accuracy'],\n",
        "        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    with open(f'{output_dir}/model_info.json', 'w') as f:\n",
        "        json.dump(model_info, f, indent=4)"
      ],
      "metadata": {
        "id": "21Mc6qnIzeWE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Loading data...\")\n",
        "    data = pd.read_csv('./sample_data/train.csv')\n",
        "\n",
        "    # Preprocess data\n",
        "    features, labels, scaler = preprocess_data(data)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Train all models\n",
        "    results = {\n",
        "        'Random Forest': train_random_forest(X_train, X_test, y_train, y_test),\n",
        "        'LightGBM': train_lightgbm(X_train, X_test, y_train, y_test),\n",
        "        'CNN': train_cnn(X_train, X_test, y_train, y_test, len(np.unique(labels)))\n",
        "    }\n",
        "\n",
        "    best_model = compare_models(results)\n",
        "\n",
        "    save_best_model(results, best_model)\n",
        "\n",
        "    print(f\"\\nBest model saved in './best_model' directory\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfr-_97yvHd_",
        "outputId": "cb495b98-f076-4ab4-80bc-9817797b8b15"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Training Random Forest...\n",
            "\n",
            "Random Forest Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     13608\n",
            "           1       0.61      0.55      0.58      6804\n",
            "           2       0.59      0.65      0.62      6804\n",
            "\n",
            "    accuracy                           0.80     27216\n",
            "   macro avg       0.74      0.73      0.73     27216\n",
            "weighted avg       0.80      0.80      0.80     27216\n",
            "\n",
            "\n",
            "Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003791 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 365\n",
            "[LightGBM] [Info] Number of data points in the train set: 108864, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score -0.693147\n",
            "[LightGBM] [Info] Start training from score -1.386294\n",
            "[LightGBM] [Info] Start training from score -1.386294\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "LightGBM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     13608\n",
            "           1       0.61      0.55      0.58      6804\n",
            "           2       0.59      0.64      0.61      6804\n",
            "\n",
            "    accuracy                           0.80     27216\n",
            "   macro avg       0.73      0.73      0.73     27216\n",
            "weighted avg       0.80      0.80      0.80     27216\n",
            "\n",
            "\n",
            "Training CNN...\n",
            "Epoch [2/10], Loss: 0.3803\n",
            "Epoch [4/10], Loss: 0.2400\n",
            "Epoch [6/10], Loss: 0.2293\n",
            "Epoch [8/10], Loss: 0.4043\n",
            "Epoch [10/10], Loss: 0.2841\n",
            "\n",
            "CNN Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     13608\n",
            "           1       0.58      0.36      0.44      6804\n",
            "           2       0.53      0.74      0.62      6804\n",
            "\n",
            "    accuracy                           0.77     27216\n",
            "   macro avg       0.70      0.70      0.69     27216\n",
            "weighted avg       0.78      0.77      0.77     27216\n",
            "\n",
            "\n",
            "Model Comparison:\n",
            "==================================================\n",
            "Random Forest Accuracy: 0.8008\n",
            "LightGBM Accuracy: 0.7988\n",
            "CNN Accuracy: 0.7742\n",
            "\n",
            "Best performing model: Random Forest with accuracy: 0.8008\n",
            "\n",
            "Best model saved in './best_model' directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI2W6LXhv5Em3mgxyN21SZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}