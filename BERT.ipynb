{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl8imfOBLMkTC+FwvQqBBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chibuezedev/ddos-detector/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install onnx onnxruntime"
      ],
      "metadata": {
        "id": "UaO_J5jrr3Aq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import onnx\n",
        "import torch.onnx\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import gc"
      ],
      "metadata": {
        "id": "GX_8WqzrdSR1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkTrafficDataset(Dataset):\n",
        "    def __init__(self, features, labels, tokenizer, max_length=32):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature_str = \" \".join(f\"{x:.2f}\" if isinstance(x, float) else str(x)\n",
        "                             for x in self.features[idx])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            feature_str,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "wiq71GJNsX0f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data_df, max_samples=5000): # increase to 100k\n",
        "    if len(data_df) > max_samples:\n",
        "        data_df = data_df.sample(n=max_samples, random_state=42)\n",
        "\n",
        "    # add all features\n",
        "    essential_features = [\n",
        "        'Packets', 'Bytes', 'Tx Packets', 'Tx Bytes',\n",
        "        'Rx Packets', 'Rx Bytes', 'tcp.srcport', 'tcp.dstport',\n",
        "        'ip.proto', 'frame.len'\n",
        "    ]\n",
        "\n",
        "    features = data_df[essential_features].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels = label_encoder.fit_transform(data_df['Label'])\n",
        "    label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "\n",
        "    return features, labels, label_mapping"
      ],
      "metadata": {
        "id": "ef3TKpFJsltH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, device, num_epochs=2, patience=2):\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # gradient accumulation\n",
        "    accumulation_steps = 4\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "            batch_count += 1\n",
        "\n",
        "            # memory mgt\n",
        "            del outputs, loss, input_ids, attention_mask, labels\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else gc.collect()\n",
        "\n",
        "            if batch_count % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_count}, Loss: {total_loss/batch_count:.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / batch_count\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
        "                break"
      ],
      "metadata": {
        "id": "vSJzxNfLsrbQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_model(model, tokenizer, label_mapping, output_path):\n",
        "    \"\"\"\n",
        "    Export the model and all necessary files for Node.js server consumption\n",
        "    \"\"\"\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # model configuration\n",
        "    config = {\n",
        "        'max_length': 32,\n",
        "        'num_labels': len(label_mapping),\n",
        "        'model_type': 'bert',\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'pad_token_id': tokenizer.pad_token_id,\n",
        "        'version': '1.0',\n",
        "        'label_mapping': label_mapping\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(output_path, 'config.json'), 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    # tokenizer files\n",
        "    tokenizer_path = os.path.join(output_path, 'tokenizer')\n",
        "    os.makedirs(tokenizer_path, exist_ok=True)\n",
        "    tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "    try:\n",
        "        # PyTorch model\n",
        "        torch_path = os.path.join(output_path, 'model.pt')\n",
        "        torch.save(model.state_dict(), torch_path)\n",
        "\n",
        "        dummy_input = tokenizer(\n",
        "            \"dummy input\",\n",
        "            add_special_tokens=True,\n",
        "            max_length=32,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            onnx_path = os.path.join(output_path, 'model.onnx')\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
        "                onnx_path,\n",
        "                input_names=['input_ids', 'attention_mask'],\n",
        "                output_names=['logits'],\n",
        "                dynamic_axes={\n",
        "                    'input_ids': {0: 'batch_size'},\n",
        "                    'attention_mask': {0: 'batch_size'},\n",
        "                    'logits': {0: 'batch_size'}\n",
        "                },\n",
        "                opset_version=14\n",
        "            )\n",
        "\n",
        "            # Verify ONNX model for nodejs server\n",
        "            onnx_model = onnx.load(onnx_path)\n",
        "            onnx.checker.check_model(onnx_model)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ONNX export failed: {str(e)}\")\n",
        "        print(\"Falling back to PyTorch model export only\")\n",
        "        onnx_path = None\n",
        "\n",
        "    shutil.make_archive(output_path, 'zip', output_path)\n",
        "\n",
        "    return {\n",
        "        'model_dir': output_path,\n",
        "        'zip_path': f\"{output_path}.zip\",\n",
        "        'onnx_path': onnx_path,\n",
        "        'torch_path': torch_path,\n",
        "        'config_path': os.path.join(output_path, 'config.json'),\n",
        "        'tokenizer_path': tokenizer_path\n",
        "    }"
      ],
      "metadata": {
        "id": "ZqS6CvCFs9vS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RmdG4JjoEXUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746f192d-0e24-499b-a2d4-9b9712e30835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Initializing BERT model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1, Batch 10, Loss: 1.0702\n",
            "Epoch 1, Batch 20, Loss: 0.9106\n",
            "Epoch 1, Batch 30, Loss: 0.7821\n",
            "Epoch 1, Batch 40, Loss: 0.7013\n",
            "Epoch 1, Batch 50, Loss: 0.6416\n",
            "Epoch 1, Batch 60, Loss: 0.5946\n",
            "Epoch 1, Batch 70, Loss: 0.5627\n",
            "Epoch 1, Batch 80, Loss: 0.5365\n",
            "Epoch 1, Batch 90, Loss: 0.5241\n",
            "Epoch 1, Batch 100, Loss: 0.5158\n",
            "Epoch 1, Batch 110, Loss: 0.5011\n",
            "Epoch 1, Batch 120, Loss: 0.4912\n",
            "Epoch 1, Batch 130, Loss: 0.4830\n",
            "Epoch 1, Batch 140, Loss: 0.4767\n",
            "Epoch 1, Batch 150, Loss: 0.4681\n",
            "Epoch 1, Batch 160, Loss: 0.4603\n",
            "Epoch 1, Batch 170, Loss: 0.4532\n",
            "Epoch 1, Batch 180, Loss: 0.4485\n",
            "Epoch 1, Batch 190, Loss: 0.4458\n",
            "Epoch 1, Batch 200, Loss: 0.4779\n",
            "Epoch 1, Batch 210, Loss: 0.4747\n",
            "Epoch 1, Batch 220, Loss: 0.4724\n",
            "Epoch 1, Batch 230, Loss: 0.4653\n",
            "Epoch 1, Batch 240, Loss: 0.4606\n",
            "Epoch 1, Batch 250, Loss: 0.4585\n",
            "Epoch 1, Batch 260, Loss: 0.4549\n",
            "Epoch 1, Batch 270, Loss: 0.4501\n",
            "Epoch 1, Batch 280, Loss: 0.4461\n",
            "Epoch 2, Batch 10, Loss: 0.3605\n",
            "Epoch 2, Batch 20, Loss: 0.3632\n",
            "Epoch 2, Batch 30, Loss: 0.3472\n",
            "Epoch 2, Batch 40, Loss: 0.3539\n",
            "Epoch 2, Batch 50, Loss: 0.3487\n",
            "Epoch 2, Batch 60, Loss: 0.3455\n",
            "Epoch 2, Batch 70, Loss: 0.3516\n",
            "Epoch 2, Batch 80, Loss: 0.3493\n",
            "Epoch 2, Batch 90, Loss: 0.3477\n",
            "Epoch 2, Batch 100, Loss: 0.3554\n",
            "Epoch 2, Batch 110, Loss: 0.3554\n",
            "Epoch 2, Batch 120, Loss: 0.3580\n",
            "Epoch 2, Batch 130, Loss: 0.3605\n",
            "Epoch 2, Batch 140, Loss: 0.3587\n",
            "Epoch 2, Batch 150, Loss: 0.3584\n",
            "Epoch 2, Batch 160, Loss: 0.3573\n",
            "Epoch 2, Batch 170, Loss: 0.3584\n",
            "Epoch 2, Batch 180, Loss: 0.3567\n",
            "Epoch 2, Batch 190, Loss: 0.3568\n",
            "Epoch 2, Batch 200, Loss: 0.3560\n",
            "Epoch 2, Batch 210, Loss: 0.3555\n",
            "Epoch 2, Batch 220, Loss: 0.3537\n",
            "Epoch 2, Batch 230, Loss: 0.3534\n",
            "Epoch 2, Batch 240, Loss: 0.3534\n",
            "Epoch 2, Batch 250, Loss: 0.3555\n",
            "Epoch 2, Batch 260, Loss: 0.3553\n",
            "Epoch 2, Batch 270, Loss: 0.3549\n",
            "Epoch 2, Batch 280, Loss: 0.3553\n",
            "Exporting model...\n",
            "\n",
            "Model export successful! Files location:\n",
            "1. Model directory: ./ddos_model/output_model\n",
            "2. ZIP archive: ./ddos_model/output_model.zip\n",
            "Training and export completed!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    data = pd.read_csv('./sample_data/train.csv', nrows=5000) #increase to 10k\n",
        "\n",
        "    features, labels, label_mapping = preprocess_data(data)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, labels, test_size=0.1, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Initialize BERT model\n",
        "    print(\"Initializing BERT model...\")\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=len(label_mapping),\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1\n",
        "    )\n",
        "\n",
        "    train_dataset = NetworkTrafficDataset(X_train, y_train, tokenizer)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    device = torch.device('cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    train_model(train_loader, model, device)\n",
        "\n",
        "    print(\"Exporting model...\")\n",
        "    try:\n",
        "        output_path = os.path.join('./ddos_model', 'output_model')\n",
        "        export_paths = export_model(model, tokenizer, label_mapping, output_path)\n",
        "        print(\"\\nModel export successful! Files location:\")\n",
        "        print(f\"1. Model directory: {export_paths['model_dir']}\")\n",
        "        print(f\"2. ZIP archive: {export_paths['zip_path']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model export: {str(e)}\")\n",
        "\n",
        "    print(\"Training and export completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}